# -*- coding: utf-8 -*-
"""SentimentAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PkVVXPXB3qAYHgPsc0XkcnrQtOIo0Ws5

## 1. Info
CS777 Big Data Analytics Term Project

Yelp Reviews Sentiment Analysis

Assiya Karatay, Euiyoung Lee

## 2. Description
In this project we will demonstrate a supervised  learning model for classification of sentiments with a sample of Yelp reviews data and vector labels over two types of sentiments.

## 3. Import libraries
"""
#import libraries
import sys
from pyspark import SparkContext
from pyspark.sql import SparkSession ,Row
from pyspark.sql.functions import col
from pyspark.sql import SQLContext
from pyspark.ml.feature import StringIndexer
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.sql.types import StructType,StructField,IntegerType,StringType,FloatType
from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer
from pyspark.ml.feature import StringIndexer
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator

import matplotlib.pyplot as plt
# %matplotlib inline

import os


spark = SparkSession.builder\
          .appName("SentimentAnalysis")\
          .getOrCreate()

schema = StructType([
    StructField("text", StringType(), True),
    StructField("target", IntegerType(), True)])

# project_folder = '/content/drive/MyDrive/CS777_BigDataAnalytics/term_project/'
project_folder = sys.argv[2]
dfTextTarget = spark.read.csv(sys.argv[1], header=False, schema=schema)
dfTextTarget = dfTextTarget.dropna()


"""## Data Split"""

# (train_set, test_set) = dfTextTarget.randomSplit([0.98, 0.01], seed = 2000)
(train_set, test_set) = dfTextTarget.randomSplit([0.8, 0.2], seed = 2000)


def eval_model(model_name,model):

  tokenizer = Tokenizer(inputCol="text", outputCol="words")
  hashtf = HashingTF(numFeatures=2**16, inputCol="words", outputCol='tf')
  idf = IDF(inputCol='tf', outputCol="features", minDocFreq=5) #minDocFreq: remove sparse terms
  label_stringIdx = StringIndexer(inputCol = "target", outputCol = "label")
  pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx,model])
  pipelineFit = pipeline.fit(train_set)

  predictions_train = pipelineFit.transform(train_set)
  predictions_test = pipelineFit.transform(test_set)

  train_accuracy = predictions_train.filter(predictions_train.label == predictions_train.prediction).count() / float(train_set.count())
  test_accuracy = predictions_test.filter(predictions_test.label == predictions_test.prediction).count() / float(test_set.count())

  evaluator = BinaryClassificationEvaluator(rawPredictionCol="rawPrediction")
  train_roc_auc = evaluator.evaluate(predictions_train)
  test_roc_auc = evaluator.evaluate(predictions_test)
  metricsList = [(model_name,train_accuracy,test_accuracy,train_roc_auc,test_roc_auc)]
  return metricsList

"""## TFIDF + LInear SVC"""

from pyspark.ml.classification import LinearSVC
lsvc = LinearSVC(maxIter=10, regParam=0.1)
lsvc_metricsList = eval_model('LinearSVC', lsvc)
spark.createDataFrame(lsvc_metricsList).write.csv(project_folder+'metrics')
