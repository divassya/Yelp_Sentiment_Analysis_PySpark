{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Info\n",
        "CS777 Big Data Analytics Term Project\n",
        "\n",
        "Yelp Reviews Sentiment Analysis\n",
        "\n",
        "Assiya Karatay, Euiyoung Lee "
      ],
      "metadata": {
        "id": "brZpV8U3dVMb"
      },
      "id": "brZpV8U3dVMb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Description\n",
        "In this project we will demonstrate a supervised  learning model for classification of sentiments with a sample of Yelp reviews data and vector labels over two types of sentiments."
      ],
      "metadata": {
        "id": "xtqmNb5nd9UB"
      },
      "id": "xtqmNb5nd9UB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Import libraries"
      ],
      "metadata": {
        "id": "BW_KWEpRgbS3"
      },
      "id": "BW_KWEpRgbS3"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.1.2 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbDceMx8Qcha",
        "outputId": "b431bbe4-8e92-4671-d4ef-0ca6c0b95e17"
      },
      "id": "zbDceMx8Qcha",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark==3.1.2\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 61 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 10.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880769 sha256=135e4ba3cabd49cd19b1bfc5af06a36d125dc920465d685bb6c835fb0660a709\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7156025d",
      "metadata": {
        "id": "7156025d"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession ,Row\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.types import StructType,StructField,IntegerType,StringType,FloatType\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsnqX3VMTc9i",
        "outputId": "abf8daeb-1f24-447f-b54e-2b014a26885a"
      },
      "id": "EsnqX3VMTc9i",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f731ae8f",
      "metadata": {
        "id": "f731ae8f",
        "outputId": "0c417cdf-ce9a-400a-fb0f-0f2a922c94d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- target: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark = SparkSession.builder\\\n",
        "          .appName(\"SentimentAnalysis\")\\\n",
        "          .getOrCreate()\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"text\", StringType(), True),\n",
        "    StructField(\"target\", IntegerType(), True)])\n",
        "\n",
        "project_folder = '/content/drive/MyDrive/CS777_BigDataAnalytics/term_project/'\n",
        "\n",
        "dfTextTarget = spark.read.csv(project_folder + 'small_preprocessed_review', \\\n",
        "                              header=False, schema=schema)\n",
        "dfTextTarget = dfTextTarget.dropna()\n",
        "dfTextTarget.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9712457f",
      "metadata": {
        "id": "9712457f",
        "outputId": "46a61104-ab2f-4753-94ce-86ff48df365d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+\n",
            "|                text|target|\n",
            "+--------------------+------+\n",
            "|horrible experien...|     0|\n",
            "|i went to the fre...|     0|\n",
            "|my phone dies at ...|     0|\n",
            "|another terrific ...|     1|\n",
            "|called on monday ...|     1|\n",
            "+--------------------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfTextTarget.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2627abc9",
      "metadata": {
        "id": "2627abc9",
        "outputId": "539a7b74-484a-4c83-c5ee-15848beaec64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69998"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "dfTextTarget.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Split"
      ],
      "metadata": {
        "id": "OuJ0m9l9jhsE"
      },
      "id": "OuJ0m9l9jhsE"
    },
    {
      "cell_type": "code",
      "source": [
        "(train_set, test_set) = dfTextTarget.randomSplit([0.8, 0.2], seed = 2000)\n",
        "test_set.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICnziSSwty-S",
        "outputId": "132b0b3f-8029-4f75-9f1c-a35e2a7f06ab"
      },
      "id": "ICnziSSwty-S",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13940"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5PWEcq2wlhc",
        "outputId": "4e440704-8cbb-462f-f990-effcb8263c2f"
      },
      "id": "R5PWEcq2wlhc",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56058"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f4dd6f9",
      "metadata": {
        "id": "4f4dd6f9"
      },
      "source": [
        "## Hashing TF - IDF -Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bb15e129",
      "metadata": {
        "id": "bb15e129"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "78163263",
      "metadata": {
        "id": "78163263",
        "outputId": "5d864ab3-f13f-486d-f4fb-d5b3f22ba180",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
            "Wall time: 10 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "def eval_model(model_name,model):\n",
        "  \n",
        "  tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "  hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
        "  idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
        "  label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
        "  pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx,model])\n",
        "  pipelineFit = pipeline.fit(train_set)\n",
        "\n",
        "  predictions_train = pipelineFit.transform(train_set)\n",
        "  predictions_test = pipelineFit.transform(test_set)\n",
        "\n",
        "  train_accuracy = predictions_train.filter(predictions_train.label == predictions_train.prediction).count() / float(train_set.count())\n",
        "  test_accuracy = predictions_test.filter(predictions_test.label == predictions_test.prediction).count() / float(test_set.count())\n",
        "\n",
        "  evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
        "  train_roc_auc = evaluator.evaluate(predictions_train)\n",
        "  test_roc_auc = evaluator.evaluate(predictions_test)\n",
        "  metricsList = [(model_name,train_accuracy,test_accuracy,train_roc_auc,test_roc_auc)]\n",
        "  return metricsList\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schemaMetrics = StructType([\\\n",
        "  StructField('model', StringType(), True),\\\n",
        "  StructField('train_accuracy', FloatType(), True),\\\n",
        "  StructField('test_accuracy', FloatType(), True),\\\n",
        "  StructField('train_ROC_AUC', FloatType(), True),\\\n",
        "  StructField('test_ROC_AUC', FloatType(), True)])\n",
        "metrics = spark.createDataFrame([], schemaMetrics)"
      ],
      "metadata": {
        "id": "xZwprf42pL1L"
      },
      "id": "xZwprf42pL1L",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "lr = LogisticRegression(maxIter=100)\n",
        "logreg_metricsList = eval_model('LogReg', lr)\n",
        "# spark.createDataFrame(logreg_metricsList).write.csv(project_folder+'metrics')\n",
        "logreg = spark.createDataFrame(logreg_metricsList, schemaMetrics)\n",
        "metrics = metrics.union(logreg)\n",
        "metrics.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDXE4fXv4tLQ",
        "outputId": "f30cc465-ce96-4795-94a4-7330be29b0a7"
      },
      "id": "bDXE4fXv4tLQ",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------+-------------+-------------+------------+\n",
            "| model|train_accuracy|test_accuracy|train_ROC_AUC|test_ROC_AUC|\n",
            "+------+--------------+-------------+-------------+------------+\n",
            "|LogReg|     0.9999643|    0.8167862|    0.9999995|   0.8584581|\n",
            "+------+--------------+-------------+-------------+------------+\n",
            "\n",
            "CPU times: user 900 ms, sys: 93.4 ms, total: 993 ms\n",
            "Wall time: 2min 3s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TFIDF + LInear SVC"
      ],
      "metadata": {
        "id": "knBdx_S00xyx"
      },
      "id": "knBdx_S00xyx"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "lsvc = LinearSVC(maxIter=10, regParam=0.1)\n",
        "lsvc_metricsList = eval_model('LinearSVC', lsvc)\n",
        "lsvc_metricsDF = spark.createDataFrame(lsvc_metricsList, schemaMetrics)\n",
        "metrics = metrics.union(lsvc_metricsDF)\n",
        "metrics.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvMo0lFv0wW1",
        "outputId": "9dfa8974-9e14-45f1-8085-a2c7db67cce2"
      },
      "id": "qvMo0lFv0wW1",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------+-------------+-------------+------------+\n",
            "|    model|train_accuracy|test_accuracy|train_ROC_AUC|test_ROC_AUC|\n",
            "+---------+--------------+-------------+-------------+------------+\n",
            "|   LogReg|     0.9999643|    0.8167862|    0.9999995|   0.8584581|\n",
            "|LinearSVC|     0.9407043|   0.90164995|   0.97813654|  0.94934887|\n",
            "+---------+--------------+-------------+-------------+------------+\n",
            "\n",
            "CPU times: user 525 ms, sys: 62 ms, total: 587 ms\n",
            "Wall time: 1min 7s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TFIDF + Decision Tree"
      ],
      "metadata": {
        "id": "aKNyHp0W3A9w"
      },
      "id": "aKNyHp0W3A9w"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier()\n",
        "dt_metricsList = eval_model('DecisionTree', dt)\n",
        "dt_metricsDF = spark.createDataFrame(dt_metricsList, schemaMetrics)\n",
        "metrics = metrics.union(dt_metricsDF)\n",
        "metrics.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4KtZ9DO5OUP",
        "outputId": "58e22f09-99ee-4419-d1c5-6e1419b70bab"
      },
      "id": "X4KtZ9DO5OUP",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+--------------+-------------+-------------+------------+\n",
            "|       model|train_accuracy|test_accuracy|train_ROC_AUC|test_ROC_AUC|\n",
            "+------------+--------------+-------------+-------------+------------+\n",
            "|      LogReg|     0.9999643|    0.8167862|    0.9999995|   0.8584581|\n",
            "|   LinearSVC|     0.9407043|   0.90164995|   0.97813654|  0.94934887|\n",
            "|DecisionTree|    0.76986337|   0.77245337|    0.6773023|   0.6837623|\n",
            "+------------+--------------+-------------+-------------+------------+\n",
            "\n",
            "CPU times: user 2.17 s, sys: 263 ms, total: 2.43 s\n",
            "Wall time: 6min 39s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4504c54",
      "metadata": {
        "id": "a4504c54"
      },
      "source": [
        "## CountVectorizer + IDF + Logistic Regression\n",
        "There's another way that you can get term frequecy for IDF (Inverse Document Freqeuncy) calculation. It is CountVectorizer in SparkML. Apart from the reversibility of the features (vocabularies), there is an important difference in how each of them filters top features. In case of HashingTF it is dimensionality reduction with possible collisions. CountVectorizer discards infrequent tokens.\n",
        "\n",
        "Let's see if performance changes if we use Countvectorizer instead of HashingTF."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def count_vectorizer_model(model_name,model):\n",
        "  \n",
        "  tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "  cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
        "  # hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
        "  idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
        "  label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
        "  pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx,model])\n",
        "  pipelineFit = pipeline.fit(train_set)\n",
        "\n",
        "  predictions_train = pipelineFit.transform(train_set)\n",
        "  predictions_test = pipelineFit.transform(test_set)\n",
        "\n",
        "  train_accuracy = predictions_train.filter(predictions_train.label == predictions_train.prediction).count() / float(train_set.count())\n",
        "  test_accuracy = predictions_test.filter(predictions_test.label == predictions_test.prediction).count() / float(test_set.count())\n",
        "\n",
        "  evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
        "  train_roc_auc = evaluator.evaluate(predictions_train)\n",
        "  test_roc_auc = evaluator.evaluate(predictions_test)\n",
        "  metricsList = [(model_name,train_accuracy,test_accuracy,train_roc_auc,test_roc_auc)]\n",
        "  \n",
        "  return metricsList\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "041J27Gx6OcM",
        "outputId": "653e6ebc-77d0-4645-8643-973d1b6ed6a4"
      },
      "id": "041J27Gx6OcM",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 0 ns, sys: 8 µs, total: 8 µs\n",
            "Wall time: 13.6 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "metricsList = count_vectorizer_model('CVIDF_LogReg', lr)\n",
        "metricsDF = spark.createDataFrame(metricsList, schemaMetrics)\n",
        "metrics = metrics.union(metricsDF)\n",
        "metrics.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vr46MUsa7_5a",
        "outputId": "b8e71d67-1fbf-48fb-ed28-b6eef52b0085"
      },
      "id": "vr46MUsa7_5a",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+--------------+-------------+-------------+------------+\n",
            "|       model|train_accuracy|test_accuracy|train_ROC_AUC|test_ROC_AUC|\n",
            "+------------+--------------+-------------+-------------+------------+\n",
            "|      LogReg|     0.9999643|    0.8167862|    0.9999995|   0.8584581|\n",
            "|   LinearSVC|     0.9407043|   0.90164995|   0.97813654|  0.94934887|\n",
            "|DecisionTree|    0.76986337|   0.77245337|    0.6773023|   0.6837623|\n",
            "|CVIDF_LogReg|     0.9999643|    0.8235294|    0.9999992|   0.8653524|\n",
            "+------------+--------------+-------------+-------------+------------+\n",
            "\n",
            "CPU times: user 341 ms, sys: 48.9 ms, total: 390 ms\n",
            "Wall time: 1min 14s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e22e8a00",
      "metadata": {
        "id": "e22e8a00"
      },
      "source": [
        "## N-gram Implementation with Chi Squared Selector\n",
        "Spark does not automatically combine features from different n-grams, so I had to use VectorAssembler in the pipeline, to combine the features I get from each n-grams.\n",
        "\n",
        "I first tried to extract around 16,000 features from unigram, bigram, trigram. This means I will get around 48,000 features in total. Then I implemented Chi Squared feature selection to reduce the features back to 16,000 in total."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ab1be70d",
      "metadata": {
        "id": "ab1be70d"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import NGram, VectorAssembler\n",
        "from pyspark.ml.feature import ChiSqSelector\n",
        "\n",
        "def build_trigrams(inputCol=[\"text\",\"target\"], n=3):\n",
        "    tokenizer = [Tokenizer(inputCol=\"text\", outputCol=\"words\")]\n",
        "    ngrams = [\n",
        "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
        "        for i in range(1, n + 1)\n",
        "    ]\n",
        "\n",
        "    cv = [\n",
        "        CountVectorizer(vocabSize=2**14,inputCol=\"{0}_grams\".format(i),\n",
        "            outputCol=\"{0}_tf\".format(i))\n",
        "        for i in range(1, n + 1)\n",
        "    ]\n",
        "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
        "\n",
        "    assembler = [VectorAssembler(\n",
        "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
        "        outputCol=\"rawFeatures\"\n",
        "    )]\n",
        "    label_stringIdx = [StringIndexer(inputCol = \"target\", outputCol = \"label\")]\n",
        "    selector = [ChiSqSelector(numTopFeatures=2**14,featuresCol='rawFeatures', outputCol=\"features\")]\n",
        "    lr = [LogisticRegression(maxIter=100)]\n",
        "    return Pipeline(stages=tokenizer + ngrams + cv + idf+ assembler + label_stringIdx+selector+lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ac35db9a",
      "metadata": {
        "id": "ac35db9a",
        "outputId": "506ba449-126c-4bb0-fedd-cfcbdc66be66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.37 s, sys: 296 ms, total: 2.66 s\n",
            "Wall time: 6min 15s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "trigram_pipelineFit = build_trigrams().fit(train_set)\n",
        "# predictions = trigram_pipelineFit.transform(val_set)\n",
        "# accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
        "# roc_auc = evaluator.evaluate(predictions)\n",
        "\n",
        "# # print accuracy, roc_auc\n",
        "# print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
        "# print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "predictions_train = trigram_pipelineFit.transform(train_set)\n",
        "predictions_test = trigram_pipelineFit.transform(test_set)\n",
        "\n",
        "train_accuracy = predictions_train.filter(predictions_train.label == predictions_train.prediction).count() / float(train_set.count())\n",
        "test_accuracy = predictions_test.filter(predictions_test.label == predictions_test.prediction).count() / float(test_set.count())\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
        "train_roc_auc = evaluator.evaluate(predictions_train)\n",
        "test_roc_auc = evaluator.evaluate(predictions_test)\n",
        "metricsList = [('Ngrams_ChiSqSelector',train_accuracy,test_accuracy,train_roc_auc,test_roc_auc)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLPtk2FBTB75",
        "outputId": "cee0434c-2578-49f4-ed26-35adcb1a1f8b"
      },
      "id": "oLPtk2FBTB75",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 845 ms, sys: 137 ms, total: 982 ms\n",
            "Wall time: 1min 39s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metricsDF = spark.createDataFrame(metricsList, schemaMetrics)\n",
        "metrics = metrics.union(metricsDF)\n",
        "metrics.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfc8iD3vTz_A",
        "outputId": "b4c72289-aa85-45d8-f428-ccc4fe255e8e"
      },
      "id": "nfc8iD3vTz_A",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------+-------------+-------------+------------+\n",
            "|               model|train_accuracy|test_accuracy|train_ROC_AUC|test_ROC_AUC|\n",
            "+--------------------+--------------+-------------+-------------+------------+\n",
            "|              LogReg|     0.9999643|    0.8167862|    0.9999995|   0.8584581|\n",
            "|           LinearSVC|     0.9407043|   0.90164995|   0.97813654|  0.94934887|\n",
            "|        DecisionTree|    0.76986337|   0.77245337|    0.6773023|   0.6837623|\n",
            "|        CVIDF_LogReg|     0.9999643|    0.8235294|    0.9999992|   0.8653524|\n",
            "|Ngrams_ChiSqSelector|     0.9999465|    0.8776901|   0.99999934|  0.92885864|\n",
            "+--------------------+--------------+-------------+-------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-gram Implementation without Chi Squared Selector"
      ],
      "metadata": {
        "id": "r4VLC_qjWby0"
      },
      "id": "r4VLC_qjWby0"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "071fe88b",
      "metadata": {
        "id": "071fe88b"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import NGram, VectorAssembler\n",
        "\n",
        "def build_ngrams_wocs(inputCol=[\"text\",\"target\"], n=3):\n",
        "    tokenizer = [Tokenizer(inputCol=\"text\", outputCol=\"words\")]\n",
        "    ngrams = [\n",
        "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
        "        for i in range(1, n + 1)\n",
        "    ]\n",
        "\n",
        "    cv = [\n",
        "        CountVectorizer(vocabSize=5460,inputCol=\"{0}_grams\".format(i),\n",
        "            outputCol=\"{0}_tf\".format(i))\n",
        "        for i in range(1, n + 1)\n",
        "    ]\n",
        "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
        "\n",
        "    assembler = [VectorAssembler(\n",
        "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
        "        outputCol=\"features\"\n",
        "    )]\n",
        "    label_stringIdx = [StringIndexer(inputCol = \"target\", outputCol = \"label\")]\n",
        "    lr = [LogisticRegression(maxIter=100)]\n",
        "    return Pipeline(stages=tokenizer + ngrams + cv + idf+ assembler + label_stringIdx+lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "ac0ed565",
      "metadata": {
        "id": "ac0ed565",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21a945a2-f64a-4ac5-9fb2-716e409a89c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.09 s, sys: 276 ms, total: 2.36 s\n",
            "Wall time: 4min 55s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "trigramwocs_pipelineFit = build_ngrams_wocs().fit(train_set)\n",
        "predictions_train = trigramwocs_pipelineFit.transform(train_set)\n",
        "predictions_test = trigramwocs_pipelineFit.transform(test_set)\n",
        "\n",
        "train_accuracy = predictions_train.filter(predictions_train.label == predictions_train.prediction).count() / float(train_set.count())\n",
        "test_accuracy = predictions_test.filter(predictions_test.label == predictions_test.prediction).count() / float(test_set.count())\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
        "train_roc_auc = evaluator.evaluate(predictions_train)\n",
        "test_roc_auc = evaluator.evaluate(predictions_test)\n",
        "metricsList = [('Ngrams_WithOut_ChiSq',train_accuracy,test_accuracy,train_roc_auc,test_roc_auc)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "2fcda2cd",
      "metadata": {
        "id": "2fcda2cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f51d6bb7-8c6d-480e-a539-a864daafce1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------+-------------+-------------+------------+\n",
            "|               model|train_accuracy|test_accuracy|train_ROC_AUC|test_ROC_AUC|\n",
            "+--------------------+--------------+-------------+-------------+------------+\n",
            "|              LogReg|     0.9999643|    0.8167862|    0.9999995|   0.8584581|\n",
            "|           LinearSVC|     0.9407043|   0.90164995|   0.97813654|  0.94934887|\n",
            "|        DecisionTree|    0.76986337|   0.77245337|    0.6773023|   0.6837623|\n",
            "|        CVIDF_LogReg|     0.9999643|    0.8235294|    0.9999992|   0.8653524|\n",
            "|Ngrams_ChiSqSelector|     0.9999465|    0.8776901|   0.99999934|  0.92885864|\n",
            "|Ngrams_WithOut_ChiSq|     0.9999822|   0.87439024|   0.99999946|   0.9283303|\n",
            "+--------------------+--------------+-------------+-------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "metricsDF = spark.createDataFrame(metricsList, schemaMetrics)\n",
        "metrics = metrics.union(metricsDF)\n",
        "metrics.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.toPandas().to_csv(project_folder+'metrics.csv')"
      ],
      "metadata": {
        "id": "Jkg94H4sqlkL"
      },
      "id": "Jkg94H4sqlkL",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The end."
      ],
      "metadata": {
        "id": "S5lvo15ton2y"
      },
      "id": "S5lvo15ton2y"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}